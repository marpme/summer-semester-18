{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classifier - Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All necessary imports at the beginning\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import fetch_mldata       \n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, explore and prepare dataset\n",
    "\n",
    "The MNIST dataset is a classic Machine Learning dataset you can get it and more information about it from the website of [Yann Lecun](http://yann.lecun.com/exdb/mnist/). MNIST contains handwrittin digits and is split into a tranings set of 60000 examples and a test set of 10000 examples. You can use the module ```sklearn``` to load the MNIST dataset in a convenient way. \n",
    "easy load, mldata.org, orginal mnist, mnist link and description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_mldata('MNIST original') #load MNIST\n",
    "X, y = mnist[\"data\"], mnist[\"target\"] #separate images and labels\n",
    "\n",
    "# shape of MNIST data\n",
    "print('digits', X.shape)\n",
    "print('labels',y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a visualization of MNIST we will plot a digit. Each line represents an image in flatten form (all pixel in a row). We have change the shape from a vector back to a matrix of the original shape to plot the image. In the case of MNIST this means a conversion of 784 pixel into 28x28 pixel. In addition we will check the label of that digit to verify it correspond to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mnist_digit(digit):\n",
    "    image = digit.reshape(28, 28)\n",
    "    plt.imshow(image, cmap='binary', interpolation='bicubic')\n",
    "\n",
    "#choose a random number, plot it and check label \n",
    "random_number = np.random.randint(1,60001)\n",
    "print('label:',y[random_number]) \n",
    "plot_mnist_digit(X[random_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a glimpse into MNIST let us explore it a bit further. Write a function ``` plot_mnist_digits(data, examples_each_row)``` that plots configurable number of examples for each class, like:\n",
    "![MNIST Examples](images/MNIST_matrix.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mnist_digits(data, examples_each_row):\n",
    "    ############################################\n",
    "    #TODO: Write a function that plots as many #    \n",
    "    #      examples of each class as defiend   #\n",
    "    #      by 'examples_each_row'              #\n",
    "    ############################################\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    \n",
    "    # Startindices of classes\n",
    "    c, indices = np.unique(y, return_index=True)\n",
    "    rows = len(c)  # rows = num of classes\n",
    "    \n",
    "    for i in range(1, rows * examples_each_row + 1):\n",
    "        fig.add_subplot(rows, examples_each_row, i)\n",
    "        plt.axis('off')\n",
    "        cur_c = (i-1) / examples_each_row # current class\n",
    "        # get start index of class and add column index\n",
    "        dataindex = indices[cur_c] + (i-1)%rows\n",
    "        # plot image\n",
    "        plot_mnist_digit(data[dataindex])\n",
    "    ############################################\n",
    "    #             END OF YOUR CODE             #\n",
    "    ############################################\n",
    "    \n",
    "plot_mnist_digits(X, examples_each_row=11)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exploring MNIST let us prepare the date for our linear classifier. First we need to separate traning and test data. Further we will shuffle the traning data to get a random distribution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and test set\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "\n",
    "# shuffle training data\n",
    "shuffle_index = np.random.permutation(60000)\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a linear classifier using Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a model to classify the MNIST dataset with the following equation:\n",
    "\n",
    "$$ L = \\frac{1}{M} \\sum_{i=1}^{M} -log\\; \\left ( \\frac{e^{h(x_j,\\Theta)}}{\\sum_{k=1}^{K}e^{h(x_k,\\Theta)}} \\right)_i + \\frac{\\lambda}{2} \\sum_{}^{} \\Theta^2, \\: with \\;\\; h(X,\\Theta) = X * \\Theta $$\n",
    "\n",
    "Using the universal equation for a loss function we can see the separate parts of that hugh equation.  \n",
    "\n",
    "$$ L = \\frac{1}{N} \\sum_i L_i(h(x_i,\\Theta),y_i) + \\lambda R(\\Theta)$$\n",
    "\n",
    "We will implement each part on its own and put them together. That way it is much easier to understand whats going on. Let us start with the score function or hypothesis:\n",
    "\n",
    "$$h(X,\\Theta) = X * \\Theta$$\n",
    "\n",
    "It is possible to calculate all score values with one matrix multiplication ([dot product](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.dot.html)) so we can use the whole traning data $X$ instead of one digit $x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_scores(X,theta):\n",
    "    ############################################\n",
    "    #TODO: Implement the hypothesis and return #\n",
    "    #      the score values for each class of  #\n",
    "    #      every digit.                        #\n",
    "    ############################################\n",
    "    return np.dot(X, theta)\n",
    "    ############################################\n",
    "    #             END OF YOUR CODE             #\n",
    "    ############################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we can define the data loss funtion $L_i$. We assume the score values are unnormalized log probabilities and we use the softmax function to calculate probabilities.\n",
    "$$ P(Y=j\\mid X=x_i) = \\frac{e^{s_j}}{\\sum_{k=1}^{K}e^{s_k}} $$\n",
    "$$ L_i = -log\\;P(Y=j\\mid X=x_i) $$\n",
    "\n",
    "Hint: If the correct classes (labels) are in a [one hot encoding](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) shape you can use a matrix multiplication to extract the correct class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support function to convert label vector into a one hot encoding matrix\n",
    "def onehot_encode_label(label):\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    label = label.reshape(len(label), 1)\n",
    "    onehot_encoded_label = onehot_encoder.fit_transform(label)\n",
    "    return onehot_encoded_label\n",
    "\n",
    "# Calculate class probability distribution for each digit from given class scores\n",
    "def softmax(class_scores):\n",
    "    ############################################\n",
    "    #TODO: Use the softmax function to compute #\n",
    "    #      class probabilties                  #\n",
    "    ############################################\n",
    "    exp_scores = np.exp(class_scores)\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    #shiftx = class_scores - np.max(class_scores)\n",
    "    #exps = np.exp(shiftx)\n",
    "    #return exps / np.sum(exps)\n",
    "    \n",
    "    ############################################\n",
    "    #             END OF YOUR CODE             #\n",
    "    ############################################\n",
    "\n",
    "# Compute data_loss L_i for the correct class\n",
    "def data_loss(class_probabilities, onehot_encode_label):\n",
    "    ############################################\n",
    "    #TODO: With hot encoded labels and class   #\n",
    "    #      probabilties calculate data loss    #\n",
    "    #      L_i                                 #\n",
    "    ############################################\n",
    "    #print(\"probs\", class_probabilities)\n",
    "    #print(\"labels\", onehot_encode_label)\n",
    "    \n",
    "    return -1.0*(np.log(class_probabilities)*onehot_encode_label)\n",
    "    ############################################\n",
    "    #             END OF YOUR CODE             #\n",
    "    ############################################\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will calculate loss $L$ using the defined functions. \n",
    "\n",
    "$$ L = \\frac{1}{M} \\sum_i L_i(h(x_i,\\Theta),y_i) + \\lambda R(\\Theta)$$\n",
    "\n",
    "Besides the loss L we will have to calculate the gradient for our loss function $L$. To minimize our loss we will need the gradient. For more information about the gradient you can use additional sources, like that good [blog post](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(X, y, theta, lam):\n",
    "    encoded_labels = onehot_encode_label(y)           # also needed for the gradient, therefore separated calculated\n",
    "    probabilities = softmax(class_scores(X,theta))    # also needed for the gradient, therefore separated calculated\n",
    "    loss_Li = data_loss(probabilities,encoded_labels) \n",
    "    \n",
    "    m = X.shape[0]  # number of training data for normalization\n",
    "    #print(\"number examples\", m)\n",
    "    #print(\"loss li\", loss_Li)\n",
    "    l2_regularization = (lam/2)*np.sum(theta*theta)   # regularization loss\n",
    "\n",
    "    ############################################\n",
    "    #TODO: Put everthing together and calculte #\n",
    "    #      loss L and gradient dL with given   #\n",
    "    #      variables.                          #\n",
    "    ############################################\n",
    "    loss = np.sum(loss_Li)/m + l2_regularization\n",
    "\n",
    "    dscores = probabilities\n",
    "    dscores -= encoded_labels\n",
    "    dscores /= m\n",
    "    \n",
    "    gradient = np.dot(X.T, dscores) + (l2_regularization * theta)\n",
    "    ############################################\n",
    "    #             END OF YOUR CODE             #\n",
    "    ############################################\n",
    "    \n",
    "    return loss,gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the cost using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(training_data, training_label, theta, lam=0.5, iterations=10, learning_rate=1e-5):\n",
    "    losses = []\n",
    "    ############################################\n",
    "    #TODO: Optimize loss with gradient descent #\n",
    "    #      update rule. Return a final model   #\n",
    "    #      and a history of loss values.       #\n",
    "    ############################################\n",
    "    for i in tqdm(range(iterations), ascii=False, desc=\"gradient descent\"):\n",
    "        (lossReturn, gradient) = lossFun(training_data, training_label, theta, lam)\n",
    "        #if i % 10 == 0 and i != 0:\n",
    "            #print \"iteration %d: loss %f\" % (i, lossReturn)\n",
    "        losses.append(lossReturn)\n",
    "        theta += -learning_rate * gradient\n",
    "    ############################################\n",
    "    #             END OF YOUR CODE             #\n",
    "    ############################################    \n",
    "    return theta, losses\n",
    "\n",
    "# Initialize learnable parameters theta \n",
    "theta = np.zeros([X_train.shape[1],len(np.unique(y_train))])\n",
    "# Start optimization with traning data, theta and optional hyperparameters\n",
    "opt_model, loss_history = gradient_descent(X_train,y_train,theta,iterations=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model\n",
    "Let us look at the optimization results. Final loss tells us how far we could reduce costs during traning process. Further we can use the first loss value as a sanity check and validate our implementation of the loss function works as intended. Recall loss value after first iteration should be $ log\\:c$ with $c$ being number of classes. To visulize the whole tranings process we can plot losss values from each iteration as a loss curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check loss after last iteration\n",
    "print('last iteration loss:',loss_history[-1])\n",
    "# Sanity check: first loss should be ln(10)\n",
    "print('first iteration loss:',loss_history[0])\n",
    "# Plot a loss curve\n",
    "plt.plot(loss_history)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations')\n",
    "\n",
    "scores = np.dot(X_train, opt_model)\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "print 'training accuracy: %.2f' % (np.mean(predicted_class == y_train))\n",
    "\n",
    "scores = np.dot(X_test, opt_model)\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "print 'test accuracy: %.2f' % (np.mean(predicted_class == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation above gave us some inside about the optimization process but did not quantified our final model. One possibility is to calculate model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelAccuracy(X,y,theta):\n",
    "    # calculate probabilities for each digit\n",
    "    probabilities = softmax(np.dot(X,theta))\n",
    "    # class with highest probability will be predicted\n",
    "    prediction = np.argmax(probabilities,axis=1)\n",
    "    # Sum all correct predictions and divied by number of data\n",
    "    #### HAD TO FIX THIS FLOATING POINT OPERATION ####\n",
    "    accuracy = (sum(prediction == y))/(X.shape[0]*1.0)\n",
    "    ##################################################\n",
    "    return accuracy\n",
    "\n",
    "print('Training accuracy: ', modelAccuracy(X_train,y_train,opt_model))\n",
    "print('Test accuracy: ', modelAccuracy(X_test,y_test,opt_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that quantification is limited. A more gerenell approach is to calculate a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) and get different model measurements from it. A good overview for model measurements is provided by the wikipedia article of [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall). We implement a confusion matrix for our model and calculate a [F1 score](https://en.wikipedia.org/wiki/F1_score) and ```print()``` it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusionMatrix(X,y,theta):\n",
    "    ############################################\n",
    "    #TODO: Calculate a confusion matrix for    # \n",
    "    #      and it.                             #\n",
    "    ############################################\n",
    "    \n",
    "    # calculate probabilities for each digit\n",
    "    probabilities = softmax(np.dot(X,theta))\n",
    "    # class with highest probability will be predicted\n",
    "    prediction = np.argmax(probabilities,axis=1)\n",
    "    \n",
    "    confusion = np.zeros((10, 10))\n",
    "       \n",
    "    for pred, exp in zip(prediction, y):\n",
    "        confusion[int(pred)][int(exp)] += 1\n",
    "    \n",
    "    return confusion\n",
    "    ############################################\n",
    "    #             END OF YOUR CODE             #\n",
    "    ############################################\n",
    "    \n",
    "def f1Score(confMatrix):\n",
    "    ############################################\n",
    "    #TODO: Calculate a F1 score from a given   #\n",
    "    #      confusion matrix.                   #\n",
    "    ############################################\n",
    "    TP = FP = TN = FN = 0\n",
    "    \n",
    "    # Calculate Truepositive, Truenegative, Falsepositive and Falsenegative by using sums from the matrix\n",
    "    TP = np.diag(confMatrix)\n",
    "    FP = confMatrix.sum(axis=0) - TP\n",
    "    FN = confMatrix.sum(axis=1) - TP\n",
    "    TN = confMatrix.sum() - (TP + FN + FP)\n",
    "    \n",
    "    # Sum up\n",
    "    TP = np.sum(TP)\n",
    "    FP = np.sum(FP)\n",
    "    FN = np.sum(FN)\n",
    "    TN = np.sum(TN)\n",
    "    \n",
    "    # Precision = tp / (tp + fp)\n",
    "    # Recall = tp / (tp + fn)\n",
    "    # F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "         \n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "    ############################################\n",
    "    #             END OF YOUR CODE             #\n",
    "    ############################################ \n",
    "\n",
    "def plotConfusionMatrix(confusion):\n",
    "    plt.matshow(confusion)\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.ylabel('True class')\n",
    "    plt.xlabel('Predicted class')\n",
    "\n",
    "confusion = confusionMatrix(X, y, theta)\n",
    "\n",
    "print(\"F1 Score: \", f1Score(confusion))\n",
    "plotConfusionMatrix(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting is to plot a part of $theta$, because you can visualize the learned templates for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.reshape(opt_model[:,0],[28,28]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
